# src/embeddings/local_embedder.py

from typing import List
from .base_embedder import BaseEmbedder
from shared_libs.config.embedding_config import LocalEmbeddingConfig
from shared_libs.utils.logger import Logger
import fastembed
import numpy as np

logger = Logger.get_logger(module_name=__name__)

class LocalEmbedder(BaseEmbedder):
    def __init__(self, config: LocalEmbeddingConfig):
        """
        Initialize the LocalEmbedder with the specified configuration.
        """
        self.model = fastembed.TextEmbedding(
            model_name=config.model_name, 
        )
        logger.info(f"LocalEmbedder initialized with model '{config.model_name}'.")

    def embed(self, text: str) -> List[float]:
        """
        Generate an embedding for a single text string.
        """
        try:
            # Wrap the text in a list and call embed
            embedding_generator = self.model.embed(text)
            
            # Convert the generator to a list and then to a numpy array
            embeddings = list(embedding_generator)
            if not embeddings:
                logger.error(f"No embeddings returned for text '{text}'.")
                return []

            embedding = np.array(embeddings)
            # logger.debug(f"Raw embedding: {embedding}")
            # Handle 2D arrays with a single embedding vector
            if embedding.ndim == 2 and embedding.shape[0] == 1:
                embedding = embedding[0]

            # Ensure that the embedding is a flat array
            if embedding.ndim != 1:
                logger.error(f"Embedding for text '{text}' is not a flat array. Got shape: {embedding.shape}")
                return []
            return embedding.tolist()
        except Exception as e:
            logger.error(f"Error during local embed: {e}")
            return []


    def batch_embed(self, texts: List[str]) -> List[List[float]]:
        """
        Generate embeddings for a list of input texts.
        """
        try:
            # Call embed and consume the generator
            embedding_generator = self.model.embed(texts)
            embeddings_list = list(embedding_generator)
            if not embeddings_list:
                logger.error("No embeddings generated by local model for batch.")
                return [[] for _ in texts]
            processed_embeddings = []
            for idx, embedding in enumerate(embeddings_list):
                # logger.debug(f"Raw embedding for text {idx}: {embedding}")
                if isinstance(embedding, np.ndarray):
                    embedding = embedding.astype(float).tolist()
                elif isinstance(embedding, (list, tuple)):
                    embedding = [float(x) if isinstance(x, (int, float)) else float(x.item()) for x in embedding]
                else:
                    logger.error(f"Unexpected embedding type in batch: {type(embedding)}")
                    embedding = []
                processed_embeddings.append(embedding)
            return processed_embeddings
        except Exception as e:
            logger.error(f"Error during batch embed: {e}")
            return [[] for _ in texts]
