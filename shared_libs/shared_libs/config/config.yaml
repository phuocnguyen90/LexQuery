# src/config/config.yaml

provider: groq  # Options: groq, google_gemini

# Embedding Models Configuration
embedding:
  default_provider: "cloud"  # Default provider for the embedding service
  mode: "api"             # Mode of interaction: 'local' or 'api'

  # Global API URL for embedding service
  api_service_url: "http://localhost:8000/embed"  # Override this in each environment as needed

  # API Endpoint-based Embedding Providers
  api_providers:
    cloud:
      service_url: "http://54.174.232.98:8000/embed"
      vector_dimension: 384

    bedrock:
      model_name: "amazon.titan-embed-text-v2:0"
      region_name: "us-east-1"
      vector_dimension: 1024

    openai_embedding:
      service_url: "https://api.openai.com/v1/embeddings"
      api_key: "${OPENAI_API_KEY}"
      model_name: "text-embedding-3-small"
      vector_dimension: 1024  

    google_gemini_embedding:
      api_key: "${GEMINI_API_KEY}"
      model_name: "text-multilingual-embedding-002"  
      vector_dimension: 768  

    ollama_embedding:
      service_url: "http://localhost:11434/embed"
      api_key: "ollama"  
      model_name: "nomic-embed-text"
      vector_dimension: 768 

  # Library Function-based Embedding Providers
  library_providers:
    local:
      # small
      # model_name: "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
      # cache_dir: "/app/models"
      # vector_dimension: 384
      # large
      model_name: "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
      cache_dir: "/app/models"
      vector_dimension: 768
    fastembed:
      library_path: "/app/libraries/fastembed.py"
      function_name: "generate_embedding"
      vector_dimension: 512
    local_gemma3:
      model_name: google/embeddinggemma-300m
      cache_dir: /models/cache/gemma3
      function_name: "generate_embedding"
      similarity: "cosine"               # or 'cosine'
      normalize: true
      inference_mode: 'auto'          # 'auto' | 'document' | 'query'
      truncate_dim: 768             # one of [768,512,256,128]
      vector_dimension: 768         # factory expects this; validated at runtime
      device: 'cuda'                  # 'cuda' | 'cpu' | omit for auto-detect
    

  # Environment Overrides
  environments:
    development:
      default_provider: "local"
      api_service_url: "http://localhost:8000/embed"
    testing:
      default_provider: "docker"
      api_service_url: "http://localhost:8001/embed"
    production:
      default_provider: "cloud"
      api_service_url: "http://54.174.232.98:8000/embed"



# LLM Models Configuration
llm:
  provider: "groq"  # Options: groq, openai, google_gemini, ollama

  groq:
    api_key: "${GROQ_API_KEY}"
    model_name: "llama-3.1-8b-instant"
    temperature: 0.7
    max_output_tokens: 2048
    embedding_model_name: "your-groq-embedding-model"  # If applicable
    # Add other Groq-specific settings if necessary

  openai:
    api_key: "${OPENAI_API_KEY}"
    model_name: "gpt-4o-mini"
    temperature: 0.7
    max_output_tokens: 4096
    # Add other OpenAI-specific settings if necessary

  google_gemini:
    api_key: "${GEMINI_API_KEY}"
    model_name: "gemini-1.5-flash"
    temperature: 0.7
    top_p: 0.95
    top_k: 64
    max_output_tokens: 2048
    # Add other Google Gemini-specific settings if necessary

  ollama:
    api_key: "ollama"  # Replace with actual API key if necessary
    model_name: "llama3.1"
    model_path: "/path/to/ollama/model"
    temperature: 0.7
    max_output_tokens: 4096
    ollama_api_url: "http://localhost:11434"
    # Add other Ollama-specific settings if necessary

# Qdrant Database Configuration
qdrant:
  api:
    api_key: "${QDRANT_API_KEY}"  # Optional if local is True
    url: "${QDRANT_URL}"          # Optional if local is True
  collection_names:
    qa_collection: "legal_qa_768"
    doc_collection: "legal_doc_768"
  distance_metric: "cosine"
  local: false  

processing:
  input_file: "src/data/raw/input.txt"
  preprocessed_file: "src/data/preprocessed/preprocessed_data.jsonl"
  processed_file: "src/data/processed/processed_data.jsonl"
  final_output_file: "src/data/processed/result.jsonl"
  document_db: "src/data/doc_db.csv"
  log_file: "src/logs/processing.log"
  delay_between_requests: 1  # in seconds
  processing: True
  schema_paths:
    pre_processing_schema: "src/config/schemas/preprocessing_schema.yaml"
    postprocessing_schema: "src/config/schemas/postprocessing_schema.yaml"
    prompts: "src/config/schemas/prompts.yaml"

ollama:
  api_key: ollama
  model_name: "llama3.1"
  model_path: "/path/to/ollama/model"
  temperature: 0.7
  max_output_tokens: 4096
  ollama_api_url: "http://localhost:11434"